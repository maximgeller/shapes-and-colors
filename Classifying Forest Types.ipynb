{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Learn With Other Kaggle Users\n",
    "\n",
    "*Author: Christian Camilo Urcuqui LÃ³pez*\n",
    "\n",
    "*Date: 27 August 2019*\n",
    "\n",
    "*GitHub: https://github.com/urcuqui/ *\n",
    "\n",
    "In this notebook I'm going to use a data science approach in order to evaluate machine learning classifiers for a friendly competition (*Classify forest types based on information about the area*).\n",
    "\n",
    "This work is divided in the next sections:\n",
    "\n",
    "+ [Data Description](#Data-Description)\n",
    "+ [Packages](#Packages)\n",
    "+ [Explore](#Explore)\n",
    "    + [Dimensions](#Dimensions)\n",
    "    + [Treatments](#Treatments)\n",
    "        + [NaN](#NaN)\n",
    "        + [Duplicated Data](#Duplicated-Data)    \n",
    "    + [Exploration Quantitative Features](#Exploration-Quantitative-Features) \n",
    "    + [Exploration Qualitative Features](#Exploration-Qualitative-Features)\n",
    "+ [Modeling](#Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last update: 2019-10-25 23:31:03.333121\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"last update: {}\".format(datetime.now())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "<div class=\"DataExplorerDescription_Container-sc-rtvgew bNXSrs\"><div class=\"DataExplorerDescription_DatasourceHeader-sc-1jjl64y hYvtkD\"><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/15767/logos/thumb76_76.png?t=2019-08-21-16-24-53\" alt=\"Learn With Other Kaggle Users source image\" class=\"DataExplorerDescription_DatasourceImage-sc-124hjw6 faQqwa\"><div class=\"DataExplorerDescription_DatasourceDetails-sc-6z9az5 cgraVO\"><div class=\"DataExplorerDescription_DatasourceOverview-sc-64u3xx dthRbg\">Classify forest types based on information about the area</div><div class=\"DataExplorerDescription_DatasourceLastUpdated-sc-1bnzx7s gFoPLV\">Last Updated: <span title=\"Tue Aug 27 2019 15:56:49 GMT-0500 (hora estÃ¡ndar de Colombia)\">a day ago</span></div></div></div><div class=\"DataExplorerDescription_Header-sc-9udzgu kagSZQ\"><div class=\"DataExplorerDescription_HeaderTitle-sc-8yzcy8 kIiVNS\">About this Competition</div><div class=\"DataExplorerDescription_HeaderRight-sc-m2iwyg fyjBEU\"></div></div><div class=\"DataExplorerDescription_Content-sc-yp9anb eysdMp\"><div class=\"markdown-converter__text--rendered data-explorer-overview-description\"><p>The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:</p>\n",
    "\n",
    "<p>1 - Spruce/Fir<br> 2 - Lodgepole Pine<br> 3 - Ponderosa Pine<br> 4 - Cottonwood/Willow<br> 5 - Aspen<br> 6 - Douglas-fir<br> 7 - Krummholz</p>\n",
    "\n",
    "<p>The training set (15120 observations) contains both features and the&nbsp;Cover_Type. The test set contains only the features. You must predict the Cover_Type&nbsp;for every row&nbsp;in the test set (565892 observations).</p>\n",
    "\n",
    "<h2>Data Fields</h2>\n",
    "\n",
    "<p><strong>Elevation</strong> - Elevation in meters<br><strong>Aspect</strong> - Aspect in degrees azimuth<br><strong>Slope</strong> - Slope in degrees<br><strong>Horizontal_Distance_To_Hydrology</strong> - Horz Dist to nearest surface water features<br><strong>Vertical_Distance_To_Hydrology</strong> - Vert Dist to nearest surface water features<br><strong>Horizontal_Distance_To_Roadways</strong> - Horz Dist to nearest roadway<br><strong>Hillshade_9am</strong> (0 to 255 index) - Hillshade index at 9am, summer solstice<br><strong>Hillshade_Noon</strong> (0 to 255 index) - Hillshade index at noon, summer solstice<br><strong>Hillshade_3pm</strong> (0 to 255 index) - Hillshade index at 3pm, summer solstice<br><strong>Horizontal_Distance_To_Fire_Points</strong> - Horz Dist to nearest wildfire ignition points<br><strong>Wilderness_Area</strong> (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation<br><strong>Soil_Type</strong> (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation<br><strong>Cover_Type</strong> (7 types, integers 1 to 7) - Forest Cover Type designation</p>\n",
    "\n",
    "<p>The wilderness areas are:</p>\n",
    "\n",
    "<p>1 - Rawah Wilderness Area<br> 2 - Neota Wilderness Area<br> 3 - Comanche Peak Wilderness Area<br> 4 - Cache la Poudre Wilderness Area</p>\n",
    "\n",
    "<p>The soil types are:</p>\n",
    "\n",
    "<p>1 Cathedral family - Rock outcrop complex, extremely stony.<br> 2 Vanet - Ratake families complex, very stony.<br> 3 Haploborolis - Rock outcrop complex, rubbly.<br> 4 Ratake family - Rock outcrop complex, rubbly.<br> 5 Vanet family - Rock outcrop complex complex, rubbly.<br> 6 Vanet - Wetmore families - Rock outcrop complex, stony.<br> 7 Gothic family.<br> 8 Supervisor - Limber families complex.<br> 9 Troutville family, very stony.<br> 10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br> 11 Bullwark - Catamount families - Rock land complex, rubbly.<br> 12 Legault family - Rock land complex, stony.<br> 13 Catamount family - Rock land - Bullwark family complex, rubbly.<br> 14 Pachic Argiborolis - Aquolis complex.<br> 15 unspecified in the USFS Soil and ELU Survey.<br> 16 Cryaquolis - Cryoborolis complex.<br> 17 Gateview family - Cryaquolis complex.<br> 18 Rogert family, very stony.<br> 19 Typic Cryaquolis - Borohemists complex.<br> 20 Typic Cryaquepts - Typic Cryaquolls complex.<br> 21 Typic Cryaquolls - Leighcan family, till substratum complex.<br> 22 Leighcan family, till substratum, extremely bouldery.<br> 23 Leighcan family, till substratum - Typic Cryaquolls complex.<br> 24 Leighcan family, extremely stony.<br> 25 Leighcan family, warm, extremely stony.<br> 26 Granile - Catamount families complex, very stony.<br> 27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br> 28 Leighcan family - Rock outcrop complex, extremely stony.<br> 29 Como - Legault families complex, extremely stony.<br> 30 Como family - Rock land - Legault family complex, extremely stony.<br> 31 Leighcan - Catamount families complex, extremely stony.<br> 32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br> 33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br> 34 Cryorthents - Rock land complex, extremely stony.<br> 35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br> 36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br> 37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br> 38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br> 39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br> 40 Moran family - Cryorthents - Rock land complex, extremely stony.</p></div></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "np.random.seed(1231)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/kaggle/input/learn-together/train.csv' does not exist: b'/kaggle/input/learn-together/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cb3dfcd215d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/kaggle/input/learn-together/train.csv\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/kaggle/input/learn-together/test.csv\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/kaggle/input/learn-together/train.csv' does not exist: b'/kaggle/input/learn-together/train.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/learn-together/train.csv\" , index_col=['Id'])\n",
    "df_test = pd.read_csv(\"/kaggle/input/learn-together/test.csv\" , index_col=['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore\n",
    "\n",
    "In this phase I'm going to see how is the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape training csv: %s\" % str(df_train.shape)) \n",
    "print(\"shape test csv: %s\" % str(df_test.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... we have more data for the testing phase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step let's see what are the feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last two chunks show that we need to transform the variables to their correct representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[:,10:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.iloc[:,10:].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[:,10:] = df_train.iloc[:,10:].astype(\"category\")\n",
    "df_test.iloc[:,10:] = df_test.iloc[:,10:].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN\n",
    "\n",
    "Let's see how many NaN we have in our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the last chunks we can assume that we don't have NaN in both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated Data\n",
    "\n",
    "Let's see if we have duplicated data in our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train.duplicated()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Quantitative Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to have a negative value in *vertical distance to hydrology* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is it possible to have a negative value in *vertical distance to hydrology* ? how many are?\n",
    "\n",
    "print(\"percent of negative values (training): \" + '%.3f' % ((df_train.loc[df_train.Vertical_Distance_To_Hydrology < 0].shape[0] / df_train.shape[0])*100))\n",
    "print(\"percent of negative values (testing): \" + '%.3f' % ((df_test.loc[df_test.Vertical_Distance_To_Hydrology < 0].shape[0]/ df_test.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_train.Vertical_Distance_To_Hydrology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_test.Vertical_Distance_To_Hydrology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wow definetily we have something here, a lot of outliers for this feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is going to happen with the other features? what are their distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_t_analyze = df_train.select_dtypes([\"float64\", \"int64\"]).columns.tolist()\n",
    "columns_t_analyze.append(\"Cover_Type\")\n",
    "plot = sns.pairplot(df_train.loc[:,columns_t_analyze], hue=\"Cover_Type\")\n",
    "plot.savefig(\"pairplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among *The last plost* we can see that some features can be used to segment the types of our forests, some of them might be:\n",
    "+ elevation\n",
    "+ horizontal_distance_to_the_hidrology\n",
    "\n",
    "As we saw the scales of each quantitave variable is significative and the boxplots and distplots allowed us to see that we have outliers (they are a lot of we can't erase them). We will need to standardize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to we have outliers in our quantitative features the idea is to use an scaler method, I'm going to use the application of a *RobustScaler* due to it is robuts to outliers (same as Quantile transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_t_analyze = df_train.select_dtypes([\"float64\", \"int64\"])\n",
    "transformer =  PowerTransformer(method='yeo-johnson').fit(columns_t_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_t_analyze = df_train.select_dtypes([\"float64\", \"int64\"])\n",
    "#columns_transformed =  RobustScaler(quantile_range=(25, 75)).fit_transform(columns_t_analyze)\n",
    "columns_transformed =  PowerTransformer(method='yeo-johnson').fit_transform(columns_t_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_transformed = pd.DataFrame(columns_transformed)\n",
    "columns_transformed.columns = columns_t_analyze.columns\n",
    "columns_transformed = pd.concat([columns_transformed, df_train.loc[:,\"Cover_Type\"]], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(columns_transformed.loc[:,columns_transformed.columns].drop(\"Cover_Type\", axis=1), columns_transformed.loc[:,'Cover_Type'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False)\n",
    "lsvc.fit(X_train, y_train)\n",
    "pred = lsvc.predict(X_test)\n",
    "print(\"LinearSVC\")\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgdc= SGDClassifier()\n",
    "sgdc.fit(X_train, y_train)\n",
    "pred = sgdc.predict(X_test)\n",
    "print(\"SGDC\")\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomfr= RandomForestClassifier()\n",
    "randomfr.fit(X_train, y_train)\n",
    "pred = randomfr.predict(X_test)\n",
    "print(\"randomfr\")\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(randomfr, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last RF we can filter some of the quantitive features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elevation, horizontal_distance_to_roadways, horizontal_distance_to_fire_points\n",
    "\n",
    "pd.DataFrame(X_new).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.concat([pd.DataFrame(X_new), df_train.loc[:,'Cover_Type']], axis=1, join='inner'), hue=\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!!! we are looking the Homoscedasticity ðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(columns_transformed.drop(columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points'], axis=1), hue=\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the last plot I could think that some quantitave have a treatment, let's work on that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "How are the distributions of our quantitatve features at test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2)\n",
    "sns.boxplot(df_train.Hillshade_3pm, ax=axs[0])\n",
    "sns.boxplot(df_test.Hillshade_3pm, ax=axs[1], color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "quan = df_train.select_dtypes([\"int\", \"float64\"])\n",
    "Q1 = quan.quantile(0.25)\n",
    "Q3 = quan.quantile(0.75)\n",
    "IQR =  Q3 - Q1\n",
    "\n",
    "(((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).sum() / quan.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing \n",
    "quan = df_test.select_dtypes([\"int\", \"float\"])\n",
    "Q1 = quan.quantile(0.25)\n",
    "Q3 = quan.quantile(0.75)\n",
    "IQR =  Q3 - Q1\n",
    "\n",
    "(((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).sum() / quan.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two tables allow us to see that the outlier percent of each quantitative feature is not high individually, so the next idea is to see what are the most important  and their impact collectively in the number of registers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the distribution of some training features without outliers\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan = df_train.select_dtypes([\"int\", \"float\"]).copy()\n",
    "Q1 = quan.quantile(0.25)\n",
    "Q3 = quan.quantile(0.75)\n",
    "IQR =  Q3 - Q1\n",
    "sns.boxplot(quan.loc[~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_3pm].Hillshade_3pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(1,8)):\n",
    "    sns.distplot(df_train.loc[(~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_3pm) & (df_train.Cover_Type == i), 'Hillshade_3pm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it looks pretty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal shape {}\".format(quan.shape[0]))\n",
    "print(\"Without outliers {}\".format(quan.loc[~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_3pm].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(quan.loc[~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Slope].Slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(1,8)):\n",
    "    sns.distplot(df_train.loc[(~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Slope) & (df_train.Cover_Type == i), 'Slope'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal shape {}\".format(quan.shape[0]))\n",
    "print(\"Without outliers {}\".format(quan.loc[~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Slope].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(quan.loc[~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_Noon].Hillshade_Noon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(1,8)):\n",
    "    sns.distplot(df_train.loc[(~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_Noon) & (df_train.Cover_Type == i), 'Hillshade_Noon'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal shape {}\".format(quan.shape[0]))\n",
    "print(\"Without outliers {}\".format(quan.loc[~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_Noon].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many data will we loose for all of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan.shape [0] - quan.loc[(~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_Noon) & (~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Slope)\n",
    "        & (~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_3pm)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... 453, these data will be important to analyze, I will keep them in another set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train[(~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_Noon) & (~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Slope)\n",
    "        & (~((quan < (Q1 - 1.5 * IQR)) | (quan > (Q3 + 1.5 * IQR))).Hillshade_3pm)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_t_analyze = df_train_copy.select_dtypes([\"float64\", \"int64\"])\n",
    "#transformer =  PowerTransformer(method='yeo-johnson').fit(columns_t_analyze)\n",
    "#columns_transformed = transformer.transform(columns_t_analyze)\n",
    "#columns_transformed = pd.DataFrame(columns_transformed)\n",
    "#columns_transformed.columns = columns_t_analyze.columns\n",
    "#columns_transformed = pd.concat([columns_transformed, df_train_copy.loc[:,\"Cover_Type\"]], axis=1, join='inner')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train_copy.loc[:,columns_t_analyze.columns], df_train_copy.loc[:,'Cover_Type'], test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomfr= RandomForestClassifier()\n",
    "randomfr.fit(X_train, y_train)\n",
    "pred = randomfr.predict(X_test)\n",
    "print(\"randomfr\")\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(randomfr, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_new).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizontal_Distance_To_Roadways\tand Elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Qualitative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objetive variable is cover_type, a feature that is categorical that has 7 values... how is it'[](http://)s distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last printing allows us to understand that we will not have problems with an objective feature that is imbalanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of categorical features, but, how many of them are really representative for our objective variable?\n",
    "\n",
    "We can use statistic functions, change them to dummy variables or use an approach of fueature hashing in oder to filter and select the categorical features,in this case I'm going to change use a feature hashing approach and analyze the feature importances using a decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.select_dtypes(\"category\").drop(columns=[\"Cover_Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "d = defaultdict(LabelEncoder)\n",
    "fit = X.apply(lambda x: d[x.name].fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = df_train.loc[:,'Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fit, Y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "print(clf)\n",
    "print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(clf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By te last process we have a number of features that are more important than the others, we can right \n",
    "now do experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative = df_train.select_dtypes(\"category\").drop(columns=[\"Cover_Type\"])\n",
    "columns_t_analyze = df_train.select_dtypes([\"float64\", \"int64\"])\n",
    "#columns_transformed =  RobustScaler(quantile_range=(25, 75)).fit_transform(columns_t_analyze)\n",
    "\n",
    "columns_transformed =  PowerTransformer(method='yeo-johnson').fit_transform(columns_t_analyze)\n",
    "columns_transformed = pd.DataFrame(columns_transformed)\n",
    "columns_transformed.columns = columns_t_analyze.columns\n",
    "columns_transformed = pd.concat([columns_transformed, df_train.loc[:,\"Cover_Type\"]], axis=1, join='inner')\n",
    "d = defaultdict(LabelEncoder)\n",
    "fit = X.apply(lambda x: d[x.name].fit_transform(x))\n",
    "fit.reset_index(drop=True, inplace=True)\n",
    "columns_transformed.reset_index(drop=True, inplace=True)\n",
    "features_preprocessing = pd.concat([fit, columns_transformed], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_preprocessing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4', 'Soil_Type10', \n",
    "                  'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type4', 'Soil_Type3', 'Soil_Type17', 'Soil_Type2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Holdout approach*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_preprocessing.loc[:,selected_columns], features_preprocessing.loc[:,'Cover_Type'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range(3, 21, 3):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    pred = neigh.predict(X_test)\n",
    "    print(\"KNeighborsClassifier {}\".format(i))\n",
    "    print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "pred = gnb.predict(X_test)\n",
    "## accuracy\n",
    "accuracy = accuracy_score(y_test,pred)\n",
    "print(\"naive_bayes\")\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "Sv=svm.SVC(gamma='scale',kernel='rbf')\n",
    "Sv.fit(X_train, y_train)\n",
    "\n",
    "pred = Sv.predict(X_test)\n",
    "# accuracy\n",
    "accuracy = accuracy_score(y_test,pred)\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "print(clf)\n",
    "print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(max_depth=10, subsample=0.8, colsample_bytree=0.7,missing=-999)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "pred = xgb.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "print(xgb)\n",
    "print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*K fold approach*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10, 13, 15],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5, 10, 20]\n",
    "        }\n",
    "\n",
    "xgb = XGBClassifier(silent=True, nthread=1)\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=4, cv=skf.split(X_train, y_train), verbose=3, random_state=1001 )\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.8, gamma=5,\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
    "              min_child_weight=10, missing=None, n_estimators=100, n_jobs=1,\n",
    "              nthread=1, objective='multi:softprob', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "              silent=True, subsample=0.8, verbosity=1)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "pred = xgb.predict(X_test)\n",
    "print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC(gamma=\"scale\")\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print('\\n All results:')\n",
    "print(clf.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(clf.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Another approach*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4', 'Soil_Type10', \n",
    "#                   'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type4', 'Soil_Type3', 'Soil_Type17', 'Soil_Type2', 'Soil_Type30', 'Soil_Type13',\n",
    "#                  'Soil_Type22', 'Soil_Type12', 'Soil_Type35', 'Soil_Type11', 'Wilderness_Area1', 'Soil_Type14']\n",
    "\n",
    "\n",
    "\n",
    "selected_columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4', 'Soil_Type10', \n",
    "                  'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type4', 'Soil_Type3', 'Soil_Type17', 'Soil_Type2', 'Soil_Type30', 'Soil_Type13',\n",
    "                 'Soil_Type22', 'Soil_Type12', 'Soil_Type35', 'Soil_Type11', 'Wilderness_Area1', 'Soil_Type14',\n",
    "                 'Wilderness_Area3', 'Soil_Type37', 'Soil_Type23', 'Soil_Type16', 'Soil_Type20', 'Soil_Type24', 'Soil_Type18', 'Wilderness_Area2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train_copy.loc[:,selected_columns], df_train_copy.loc[:,'Cover_Type'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomfr= RandomForestClassifier()\n",
    "randomfr.fit(X_train, y_train)\n",
    "pred = randomfr.predict(X_test)\n",
    "print(\"randomfr\")\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range(3, 21, 3):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    pred = neigh.predict(X_test)\n",
    "    print(\"KNeighborsClassifier {}\".format(i))\n",
    "    print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "random_grid = {'bootstrap': [True, False],\n",
    "               'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'n_estimators': [130, 180, 230]}\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "rf = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=random_grid, n_iter=param_comb, scoring='accuracy', n_jobs=4, cv=5, verbose=3, random_state=1001 )\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_estimator_.fit(X_train, y_train)\n",
    "pred = random_search.best_estimator_.predict(X_test)\n",
    "print(\"randomfr\")\n",
    "print(classification_report(y_test,pred, labels=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualitative = df_train_copy.select_dtypes(\"category\").drop(columns=[\"Cover_Type\"])\n",
    "# columns_t_analyze = df_train.select_dtypes([\"float64\", \"int64\"])\n",
    "# #columns_transformed =  RobustScaler(quantile_range=(25, 75)).fit_transform(columns_t_analyze)\n",
    "\n",
    "# columns_transformed = pd.concat([columns_t_analyze, df_train_copy.loc[:,\"Cover_Type\"]], axis=1, join='inner')\n",
    "# d = defaultdict(LabelEncoder)\n",
    "# fit = X.apply(lambda x: d[x.name].fit_transform(x))\n",
    "# fit.reset_index(drop=True, inplace=True)\n",
    "# columns_transformed.reset_index(drop=True, inplace=True)\n",
    "# features_preprocessing = pd.concat([fit, columns_transformed], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10, 13, 15],\n",
    "#         'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "#         'subsample': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'max_depth': [3, 4, 5, 10, 20]\n",
    "#         }\n",
    "\n",
    "# xgb = XGBClassifier(silent=True, nthread=1)\n",
    "# folds = 3\n",
    "# param_comb = 5\n",
    "\n",
    "# #skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=4, cv=4, verbose=3, random_state=1001 )\n",
    "\n",
    "# random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = random_search.best_estimator_\n",
    "\n",
    "# xgb.fit(X_train, y_train)\n",
    "# pred = xgb.predict(X_test)\n",
    "# print(classification_report(pred, y_test, labels=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tranining with all data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 experiment\n",
    "#neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "#neigh.fit(features_preprocessing.loc[:,selected_columns], features_preprocessing.loc[:,'Cover_Type'])\n",
    "# 2 experiment\n",
    "# xgb = XGBClassifier(max_depth=10, subsample=0.8, colsample_bytree=0.7,missing=-999)\n",
    "# xgb.fit(features_preprocessing.loc[:,selected_columns], features_preprocessing.loc[:,'Cover_Type'])\n",
    "# 3 experiment \n",
    "\n",
    "selected_columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4', 'Soil_Type10', \n",
    "                  'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type4', 'Soil_Type3', 'Soil_Type17', 'Soil_Type2', 'Soil_Type30', 'Soil_Type13',\n",
    "                 'Soil_Type22', 'Soil_Type12', 'Soil_Type35', 'Soil_Type11', 'Wilderness_Area1', 'Soil_Type14',\n",
    "                 'Wilderness_Area3', 'Soil_Type37', 'Soil_Type23', 'Soil_Type16', 'Soil_Type20', 'Soil_Type24', 'Soil_Type18', 'Wilderness_Area2']\n",
    "\n",
    "\n",
    "# 4 experiment\n",
    "\n",
    "random_search.best_estimator_.fit(df_train_copy.loc[:,selected_columns], df_train_copy.loc[:,'Cover_Type'])\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# randomfr= RandomForestClassifier()\n",
    "# randomfr.fit(df_train_copy.loc[:,selected_columns], df_train_copy.loc[:,'Cover_Type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_t_analyze = df_test.select_dtypes([\"float64\", \"int64\"])\n",
    "columns_transformed =  transformer.transform(columns_t_analyze)\n",
    "columns_transformed = pd.DataFrame(columns_transformed)\n",
    "columns_transformed.columns = columns_t_analyze.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "d = defaultdict(LabelEncoder)\n",
    "X = df_test.select_dtypes(\"category\")\n",
    "fit = X.apply(lambda x: d[x.name].fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.reset_index(drop=True, inplace=True)\n",
    "columns_transformed.reset_index(drop=True, inplace=True)\n",
    "features_test_preprocessing = pd.concat([columns_transformed, fit], axis=1, join='inner')\n",
    "features_test_preprocessing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_preprocessing.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4', 'Soil_Type10', \n",
    "#                  'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type4']\n",
    "\n",
    "results = xgb.predict(features_test_preprocessing.loc[:,selected_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4', 'Soil_Type10', \n",
    "                  'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type4', 'Soil_Type3', 'Soil_Type17', 'Soil_Type2']\n",
    "\n",
    "results = randomfr.predict(df_test.loc[:,selected_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=[\"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area4', 'Soil_Type10', \n",
    "                  'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type4', 'Soil_Type3', 'Soil_Type17', 'Soil_Type2', 'Soil_Type30', 'Soil_Type13',\n",
    "                 'Soil_Type22', 'Soil_Type12', 'Soil_Type35', 'Soil_Type11', 'Wilderness_Area1', 'Soil_Type14',\n",
    "                 'Wilderness_Area3', 'Soil_Type37', 'Soil_Type23', 'Soil_Type16', 'Soil_Type20', 'Soil_Type24', 'Soil_Type18', 'Wilderness_Area2']\n",
    "\n",
    "results = random_search.best_estimator_.predict(df_test.loc[:,selected_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': df_test.index,\n",
    "                       'Cover_Type': results})\n",
    "output.to_csv('submission_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "+ https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63\n",
    "+ https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/\n",
    "+ https://pbpython.com/categorical-encoding.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
